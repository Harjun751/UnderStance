{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688c491-f4a6-4f09-8e30-fe3f182ebcfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-together\n",
    "!pip install python-dotenv\n",
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d750c-3a2c-457d-a948-e629d16abdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8342d065-9e6c-4ad6-9a09-cadfeb8d316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79626def-3a51-4bc8-b8e0-fef4d0f84075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "API_TOKEN: Optional[str] = os.getenv(\"LLM_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d1fed6-0f68-4942-b775-4739eea6d88e",
   "metadata": {},
   "source": [
    "### Basic together.ai example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bcd347-3f64-4d38-8942-fdd59b6ddc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = TogetherLLM(\n",
    "    # DONT CHANGE THE MODEL\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
    "    api_key=API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033453f4-3b11-4e44-8093-450cb74e0898",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resp = llm.complete(\"WHAT THE\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d6f7-c95c-4f11-9557-dd0dd52e60f5",
   "metadata": {},
   "source": [
    "# Overall Workflow (i think?)\n",
    "1. Manifesto data > PDF Data Connector (or any applicable connector)\n",
    "2. Structured Data Extraction on manifesto to issues/questions\n",
    "3. Convert extracted data into SQL and then insert into database\n",
    "\n",
    "Then, augment model with RAG from database -> Make model have access to our issue/stances database so that it can see the current issues/stances and fill in gaps or ignore if we already have the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38a43d-fee9-41c0-8ff9-2ec2849958a9",
   "metadata": {},
   "source": [
    "## Pydantic Classes for SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee538153-ac1f-40b1-bdc9-d89b584312a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Deque, List, Optional, Tuple\n",
    "\n",
    "# Need to somehow cross check category\n",
    "class Issue(BaseModel):\n",
    "    \"\"\"A single issue, problem, or event that can be addressed by political parties, typically for them to form policy around.\"\"\"\n",
    "    \n",
    "    description: str = Field(description=\"A long (max 300 characters) description of the issue at hand, it should not be tied to any political party and simply be a description of the issue discussed\")\n",
    "    summary: str = Field(description=\"A short (max 50 characters), concise, summary of the issue. again, it should not be tied to any political party and simply be a description of the issue discussed\")\n",
    "\n",
    "class Stance(BaseModel):\n",
    "    \"\"\"A political party's stance on an issue\"\"\"\n",
    "    \n",
    "    issue: Issue = Field(description = \"The issue that is being discussed\")\n",
    "    stand: bool = Field(description = \"Whether the party disagrees or agrees with the contents of the issue\")\n",
    "    reason: str = Field(description = \"A description on why the bparty has this stance\")\n",
    "    party: str = Field(description = \"The name of the political party that holds this stance\")    \n",
    "\n",
    "class IssueList(BaseModel):\n",
    "    \"A list of issues addressed in the context provided\"\n",
    "\n",
    "    issues: List[Issue]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a67c9-e798-4a19-b6d8-b9316f9bf644",
   "metadata": {},
   "source": [
    "## Reading WP manifesto data (from .html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a60d3dd-c4d6-4c6c-ab77-1387ae7e23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/WP-Manifesto.txt\"]\n",
    ")\n",
    "data = reader.load_data()\n",
    "text = data[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c5267-876e-4286-94a7-0d6291ca0f33",
   "metadata": {},
   "source": [
    "### Pre-process data\n",
    "TODO: We have to remove this step - this is simply truncating output. Find another way to make data fit in context window (condense?) or find a way to increase context window (pay money? lol?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce5d2932-dcb2-4930-8bf2-65821099c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "# the free APIs only allow 8193 tokens maximum\n",
    "# We set the output to 300 tokens, leaving us with 8193-300 tokens left\n",
    "# Then we multiply by 3/4 to roughly get the amount of words that should be in the input\n",
    "approx_words = int((8193-1000) * 0.75)\n",
    "# Cut down the text to the approximate words\n",
    "text = ' '.join(data[0].text.split()[:approx_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84861505-2808-43af-a0ec-22a92b87f9dd",
   "metadata": {},
   "source": [
    "## Instantiating the Structured LLM (custom wrapper over together.ai LLM since the together.api endpoint via llamaindex doesn't work with structured json for some reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5759142c-382b-4068-878e-2306f1087eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "import together\n",
    "from llama_index.core import SimpleDirectoryReader, SummaryIndex\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "class CustomTogetherLLM(CustomLLM):\n",
    "    context_window: int = 8193 # check this\n",
    "    num_output: int = 800 #check this\n",
    "    model_name: str = \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\"\n",
    "    client: together.client.Together = together.Together(api_key=API_TOKEN)\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "            is_chat_model=False\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        # TODO: Error handling\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "            max_tokens = self.num_output\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "        return CompletionResponse(text=response.choices[0].message.content)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "        self, prompt: str, **kwargs: Any\n",
    "    ) -> CompletionResponseGen:\n",
    "        raise NotImplementedError(\"no streams.\")\n",
    "\n",
    "    \n",
    "Settings.llm = CustomTogetherLLM()\n",
    "Settings.context_window = 8193\n",
    "Settings.num_output = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db69b565-3b3a-44ca-b9bd-7296b4ae132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CustomTogetherLLM()\n",
    "sllm = llm.as_structured_llm(IssueList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa84628-c98e-4764-8290-f886467cb209",
   "metadata": {},
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c6469a95-efa8-4596-83c6-24a0de0d04f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"issues\": [\n",
      "    {\n",
      "      \"description\": \"Unemployment insurance to support retrenched workers\",\n",
      "      \"summary\": \"Unemployment Insurance\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Legislation to prohibit employment discrimination based on race, gender, age, and nationality\",\n",
      "      \"summary\": \"Anti-Discrimination Law\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Mandatory consideration of flexible work arrangements for companies with over 20 employees\",\n",
      "      \"summary\": \"Flexible Work Arrangements\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Introduction of a shared parental leave scheme for 24 weeks of government-paid leave\",\n",
      "      \"summary\": \"Shared Parental Leave\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Credential assessments for Employment Pass and S Pass job applicants\",\n",
      "      \"summary\": \"Work Pass Credential Assessments\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Expansion of the public rental scheme to cater to changing aspirations of Singaporeans\",\n",
      "      \"summary\": \"Public Rental Scheme\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Rental subsidies for rental flat applicants who cannot find alternative temporary housing\",\n",
      "      \"summary\": \"Rental Subsidies\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Reducing wait times for BTO flats to less than three years\",\n",
      "      \"summary\": \"BTO Wait Times\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Introduction of a Family Doctor Pairing Scheme for better care of families\",\n",
      "      \"summary\": \"Family Doctor Scheme\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Greater responsibility for banks to prevent scams and compensate victims\",\n",
      "      \"summary\": \"Scam Victim Protection\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Widening access to justice through a Public Defender's Office\",\n",
      "      \"summary\": \"Access to Justice\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Adjusting national servicemen's pay to account for inflationary pressures\",\n",
      "      \"summary\": \"National Servicemen's Pay\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Allowing Muslim nurses to wear the tudung at work\",\n",
      "      \"summary\": \"Tudung at Workplace\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Waiving ERP administrative fees for motorists who pay within a certain timeframe\",\n",
      "      \"summary\": \"ERP Fee Waiver\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Developing a national hydrogen strategy to support decarbonisation efforts\",\n",
      "      \"summary\": \"National Hydrogen Strategy\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Establishing a Standing Parliamentary Committee on the Cost of Living\",\n",
      "      \"summary\": \"Cost of Living Committee\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Exploring alternative revenue channels to the GST\",\n",
      "      \"summary\": \"Alternative Revenue Channels\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Exempting essential items from GST to ease the effects of inflation\",\n",
      "      \"summary\": \"GST Exemptions\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Implementing a redundancy insurance scheme to support workers facing job insecurity\",\n",
      "      \"summary\": \"Redundancy Insurance\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Mandating retrenchment benefits for private sector establishments with at least 25 employees\",\n",
      "      \"summary\": \"Retrenchment Benefits\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Introducing a national minimum wage to ensure all Singaporeans are paid a living wage\",\n",
      "      \"summary\": \"National Minimum Wage\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Implementing tiered pricing for electricity usage to lower bills for households\",\n",
      "      \"summary\": \"Tiered Pricing for Electricity\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Breaking down water pricing tiers to better reflect consumption patterns\",\n",
      "      \"summary\": \"Water Pricing Tiers\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"Reducing healthcare costs for vulnerable groups, including the elderly and persons with disabilities\",\n",
      "      \"summary\": \"Healthcare Cost Reduction\"\n",
      "    },\n",
      "    {\n",
      "      \"description\": \"L\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for IssueList\n  Invalid JSON: EOF while parsing a list at line 98 column 5 [type=json_invalid, input_value='{\\n  \"issues\": [\\n    {\\... Cost Reduction\"\\n    }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43msllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index/core/llms/callbacks.py:435\u001b[39m, in \u001b[36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[39m\u001b[34m(_self, *args, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    427\u001b[39m     CBEventType.LLM,\n\u001b[32m    428\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    432\u001b[39m     },\n\u001b[32m    433\u001b[39m )\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    437\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    438\u001b[39m         CBEventType.LLM,\n\u001b[32m    439\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    440\u001b[39m         event_id=event_id,\n\u001b[32m    441\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index/core/llms/structured_llm.py:107\u001b[39m, in \u001b[36mStructuredLLM.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;129m@llm_completion_callback\u001b[39m()\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcomplete\u001b[39m(\n\u001b[32m    104\u001b[39m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs: Any\n\u001b[32m    105\u001b[39m ) -> CompletionResponse:\n\u001b[32m    106\u001b[39m     complete_fn = chat_to_completion_decorator(\u001b[38;5;28mself\u001b[39m.chat)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplete_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index/core/base/llms/generic_utils.py:184\u001b[39m, in \u001b[36mchat_to_completion_decorator.<locals>.wrapper\u001b[39m\u001b[34m(prompt, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> CompletionResponse:\n\u001b[32m    182\u001b[39m     \u001b[38;5;66;03m# normalize input\u001b[39;00m\n\u001b[32m    183\u001b[39m     messages = prompt_to_messages(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     chat_response = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# normalize output\u001b[39;00m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_response_to_completion_response(chat_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index/core/llms/callbacks.py:175\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    167\u001b[39m     CBEventType.LLM,\n\u001b[32m    168\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     },\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    177\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    178\u001b[39m         CBEventType.LLM,\n\u001b[32m    179\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    180\u001b[39m         event_id=event_id,\n\u001b[32m    181\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index/core/llms/structured_llm.py:75\u001b[39m, in \u001b[36mStructuredLLM.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[32m     68\u001b[39m \n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# NOTE: we are wrapping existing messages in a ChatPromptTemplate to\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# make this work with our FunctionCallingProgram, even though\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# the messages don't technically have any variables (they are already formatted)\u001b[39;00m\n\u001b[32m     73\u001b[39m chat_prompt = ChatPromptTemplate(message_templates=messages)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ChatResponse(\n\u001b[32m     79\u001b[39m     message=ChatMessage(\n\u001b[32m     80\u001b[39m         role=MessageRole.ASSISTANT, content=output.model_dump_json()\n\u001b[32m     81\u001b[39m     ),\n\u001b[32m     82\u001b[39m     raw=output,\n\u001b[32m     83\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index/core/llms/llm.py:378\u001b[39m, in \u001b[36mLLM.structured_predict\u001b[39m\u001b[34m(self, output_cls, prompt, llm_kwargs, **prompt_args)\u001b[39m\n\u001b[32m    366\u001b[39m dispatcher.event(\n\u001b[32m    367\u001b[39m     LLMStructuredPredictStartEvent(\n\u001b[32m    368\u001b[39m         output_cls=output_cls, template=prompt, template_args=prompt_args\n\u001b[32m    369\u001b[39m     )\n\u001b[32m    370\u001b[39m )\n\u001b[32m    371\u001b[39m program = get_program_for_llm(\n\u001b[32m    372\u001b[39m     output_cls,\n\u001b[32m    373\u001b[39m     prompt,\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     pydantic_program_mode=\u001b[38;5;28mself\u001b[39m.pydantic_program_mode,\n\u001b[32m    376\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m result = \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprompt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    381\u001b[39m dispatcher.event(LLMStructuredPredictEndEvent(output=result))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index/core/program/llm_program.py:102\u001b[39m, in \u001b[36mLLMTextCompletionProgram.__call__\u001b[39m\u001b[34m(self, llm_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._llm.complete(formatted_prompt, **llm_kwargs)\n\u001b[32m    100\u001b[39m     raw_output = response.text\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mself\u001b[39m._output_cls):\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput parser returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._output_cls\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:319\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    322\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/llama_index/core/output_parsers/pydantic.py:63\u001b[39m, in \u001b[36mPydanticOutputParser.parse\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse, validate, and correct errors programmatically.\"\"\"\u001b[39;00m\n\u001b[32m     62\u001b[39m json_str = extract_json_str(text)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UnderStance/StanceGeneration/jupyter-venv/lib/python3.13/site-packages/pydantic/main.py:746\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, context, by_alias, by_name)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    742\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    743\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    744\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for IssueList\n  Invalid JSON: EOF while parsing a list at line 98 column 5 [type=json_invalid, input_value='{\\n  \"issues\": [\\n    {\\... Cost Reduction\"\\n    }', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid"
     ]
    }
   ],
   "source": [
    "response = sllm.complete(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b212fe-936a-4d6b-9e11-eb30e345e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter-venv)",
   "language": "python",
   "name": "jupyter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
